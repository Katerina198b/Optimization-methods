{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замятина Екатериан 476 группа: 3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = 100(x_2 - x_1^2)^2 + (1-x_1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that x* = (1,1)^T is the only local minimizer of this function, and that the Hessian\n",
    "matrix at that point is positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial step length = 1\n",
    "\n",
    "\n",
    "a) Initial point x_0=(1.2,1.2)\n",
    "\n",
    "\n",
    "b) Initial point x_0 = (-1.2, -1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest descents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_step = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla f(x) = \\left(\\begin{array}{crl}\n",
    "-400x_1(x_2 - x_1^2) + -2(1-x_1)\\\\\n",
    "200(x_2 - x_1^2)\n",
    "\\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = np.array(x)\n",
    "    return 100*(x[1] - x[0]*x[0])**2 + (1 - x[0])**2\n",
    "\n",
    "def gradient(x):\n",
    "    grad = []\n",
    "    grad.append(-400*(x[1] - x[0]**2)*x[0] - 2*(1 - x[0]))\n",
    "    grad.append(200*(x[1] - x[0]**2))\n",
    "    grad = np.array(grad)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most line search algorithms require\n",
    "p_k to be a descent direction —one for which\n",
    "$p_k^T\\nabla f_k < 0$\n",
    "0—because this property guarantees that the function f can be reduced along this direction, as discussed in the previous chapter. Moreover, the search direction often has the form\n",
    "$$p_k = - B_k^{-1}\\nabla f_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $B_k$ is a symmetric and nonsingular matrix. In the steepest descent method B_k\n",
    "is simply the identity matrix I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p(x):\n",
    "    return gradient(x)* (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def step(current_step, x):\n",
    "    while ((f(x) - f(x+current_step*p(x))) < -0.8*current_step*np.dot(gradient(x), p(x))):\n",
    "        current_step = current_step*0.8\n",
    "    return current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(initial_step, initial_x, max_number_iterations, eps = 10**(-9)):\n",
    "    current_x = initial_x\n",
    "    current_step = initial_step\n",
    "    for i in range(max_number_iterations):\n",
    "        grad = gradient(current_x)\n",
    "        current_step = step(current_step, current_x)\n",
    "        if (i>0):\n",
    "            if np.dot(current_x - previous_x, current_x - previous_x) < eps*eps:\n",
    "                return {\"x\": current_x, \"f(x)\": f(current_x), \"iteraion number\": i}   \n",
    "        previous_x = current_x\n",
    "        current_x = current_x - current_step * grad\n",
    "        #print grad, current_x, current_step\n",
    "    return {\"x\": current_x, \"f(x)\": f(current_x), \"iteraion number\": i}         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Initial point x_0=(1.2,1.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f(x)': 2.1412138714529115e-11,\n",
       " 'iteraion number': 106480,\n",
       " 'x': array([ 1.00000462,  1.00000927])}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_x = 1.2*np.ones(2)\n",
    "gradient_descent(initial_step, initial_x, max_number_iterations=500000) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Initial point x_0=(-1.2,-1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f(x)': 3.3455473986602755e-11,\n",
       " 'iteraion number': 140876,\n",
       " 'x': array([ 0.99999422,  0.99998842])}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_x = -1.2*np.ones(2)\n",
    "gradient_descent(initial_step, initial_x, max_number_iterations=500000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla^2 f(x) = \\left(\\begin{array}{crl}\n",
    "400(x_1^2 - x_2) + 800x_1^2 + 2& -400x_1\\\\\n",
    "-400x_1& 200\n",
    "\\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hessian(x):\n",
    "    return np.array([[-400*(x[1] - np.power(x[0], 2)) + 800*np.power(x[0], 2) + 2, -400*x[0]],[-400*x[0], 200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton_method(current_x, current_step, max_number_iterations = 500000, eps = 10**(-6)):\n",
    "    current_f = f(current_x)\n",
    "    for i in range(max_number_iterations):\n",
    "        if (i > 0):\n",
    "            if np.dot(current_x - previous_x, current_x - previous_x) < eps*eps:\n",
    "                return {\"x\": current_x, \"f(x)\": f(current_x), \"iteraion number\": i}   \n",
    "        previous_x = current_x\n",
    "        current_x = previous_x - current_step* np.dot(np.linalg.inv(hessian(current_x)), gradient(current_x))\n",
    "        current_step = step(current_step, current_x)\n",
    "    return {\"x\": current_x, \"f(x)\": f(current_x), \"iteraion number\": i}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Initial point $x_0=(1.2,1.2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f(x)': 3.7593108043332655e-06,\n",
       " 'iteraion number': 34919,\n",
       " 'x': array([ 1.00188224,  1.00372151])}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_x = 1.2*np.ones(2)\n",
    "hessian(initial_x)\n",
    "newton_method(initial_x,initial_step, max_number_iterations=500000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Initial point $x_0=(-1.2,-1.2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f(x)': 5.3658925190103501e-06,\n",
       " 'iteraion number': 67393,\n",
       " 'x': array([ 0.99768421,  0.99536829])}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_x = -1.2*np.ones(2)\n",
    "hessian(initial_x)\n",
    "newton_method(initial_x,initial_step, max_number_iterations=500000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
